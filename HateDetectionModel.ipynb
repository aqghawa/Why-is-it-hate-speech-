{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertModel,BertForSequenceClassification\nfrom tqdm.notebook import tqdm\nfrom torch import nn\nimport torch.nn.functional as F\nimport transformers\nimport random\nfrom sklearn.metrics import f1_score \nfrom sklearn.metrics import accuracy_score\nimport gc\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import f1_score\nimport torchmetrics\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.metrics import auc","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-15T01:14:30.484589Z","iopub.execute_input":"2022-12-15T01:14:30.486012Z","iopub.status.idle":"2022-12-15T01:14:35.539155Z","shell.execute_reply.started":"2022-12-15T01:14:30.485883Z","shell.execute_reply":"2022-12-15T01:14:35.537907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Downloading Tokenizer and Model","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertModel.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:14:35.544959Z","iopub.execute_input":"2022-12-15T01:14:35.547951Z","iopub.status.idle":"2022-12-15T01:14:52.133469Z","shell.execute_reply.started":"2022-12-15T01:14:35.547909Z","shell.execute_reply":"2022-12-15T01:14:52.132465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"hatexplain\")","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:14:52.134992Z","iopub.execute_input":"2022-12-15T01:14:52.135352Z","iopub.status.idle":"2022-12-15T01:15:00.927217Z","shell.execute_reply.started":"2022-12-15T01:14:52.135305Z","shell.execute_reply":"2022-12-15T01:15:00.926257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-processing","metadata":{}},{"cell_type":"code","source":"# Create Dataset Class\nclass HateXplainDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        dataset={}\n        \n        if len(data[\"annotators\"]) != len(data[\"rationales\"]) or len(data[\"annotators\"]) != len(data[\"post_tokens\"]):\n            raise AttributeError(\"Incorrect length in data_dict\")\n        \n        rationale  = data['rationales']\n        tokens = data['post_tokens']\n        \n        rationale =[[np.array(x) for x in multiple_lists] for multiple_lists in rationale]\n        rat = [[str(int(round(np.mean(k)))) for k in zip(*arrays)] for arrays in rationale]\n        \n        for i in range(len(rat)):\n            if not rat[i]:\n                rat[i] = ['0' for k in range(len(tokens[i]))]\n        x=[x[\"label\"] for x in data[\"annotators\"]]\n        labels=[]\n        for i in x:\n            labels.append(max(set(i), key=i.count))\n            \n        dataset[\"post_tokens\"]=data[\"post_tokens\"]\n        dataset[\"rationales\"]=rat\n        dataset[\"labels\"]=labels\n        self.data = dataset\n\n    def __len__(self):\n        dd = self.data\n        return len(dd[\"post_tokens\"])\n\n    def __getitem__(self, idx):\n        dd = self.data\n        return dd[\"post_tokens\"][idx], dd[\"rationales\"][idx], dd[\"labels\"][idx]\n    ","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:00.929715Z","iopub.execute_input":"2022-12-15T01:15:00.930466Z","iopub.status.idle":"2022-12-15T01:15:00.944489Z","shell.execute_reply.started":"2022-12-15T01:15:00.930424Z","shell.execute_reply":"2022-12-15T01:15:00.943202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create Loader builder\ndef build_loader(data_dict: dict, batch_size: int = 64, shuffle: bool = False):\n    ls=list(zip(data_dict[\"post_tokens\"],data_dict[\"rationales\"],data_dict[\"labels\"]))\n    def loader():\n        if shuffle:\n            random.shuffle(ls)\n        for i in range(0, len(ls), batch_size):\n            batch=ls[i:min(i + batch_size, len(ls))]\n            tok, rat, lab = zip(*batch)\n            yield tok,rat,lab\n\n    return loader","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:00.945926Z","iopub.execute_input":"2022-12-15T01:15:00.946622Z","iopub.status.idle":"2022-12-15T01:15:00.958351Z","shell.execute_reply.started":"2022-12-15T01:15:00.946585Z","shell.execute_reply":"2022-12-15T01:15:00.957325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Since the tokenizer might expand some words, this functions \n# expands the corresponding rationales\ndef get_token_rationales(token_ls: \"list[list[str]]\", rationale_ls: \"list[list[int]]\"):\n    rat=[]\n    sep_id=[]\n    for i in range(0,len(token_ls)):\n        temp_rat=[]\n        for j in range(len(token_ls[i])):\n            ids=tokenizer(token_ls[i][j])[\"input_ids\"]\n            temp_rat=temp_rat+[rationale_ls[i][j]]*len(ids[1:-1])\n        rat.append(temp_rat)\n        sep_id.append(len(rat[i])+1)\n    return rat,sep_id","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:00.995007Z","iopub.execute_input":"2022-12-15T01:15:00.995272Z","iopub.status.idle":"2022-12-15T01:15:01.003530Z","shell.execute_reply.started":"2022-12-15T01:15:00.995236Z","shell.execute_reply":"2022-12-15T01:15:01.002532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Takes embedded sequences and rationales and generates masked output, p is the percentage masking\n# p defaulted to 15% since bert's MLM is best at 15%\n# Output\n# - comb: sum of sentence sequence and rationale embeddings\n# - masked_indices: list of the indices of masked rationales for each sequence sentence \ndef mask_emb(seq,rat,sep_id,p=0.15):\n    rat=rat.detach().clone()\n    rat[:,0,:]=0\n    masked_indices=[]\n    for i in range(len(seq)):\n        rat[i,sep_id[i]:]=0\n        x=round(p*(sep_id[i]-1))\n        temp_masked_indices = torch.randperm(sep_id[i]-1)[:x]+1\n        rat[i,temp_masked_indices]=0\n        #print(temp_masked_indices)\n        #print(rat[i,temp_masked_indices])\n        masked_indices.append(temp_masked_indices)\n    comb=seq+rat\n    return comb,masked_indices","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:01.026587Z","iopub.execute_input":"2022-12-15T01:15:01.027327Z","iopub.status.idle":"2022-12-15T01:15:01.039300Z","shell.execute_reply.started":"2022-12-15T01:15:01.027289Z","shell.execute_reply":"2022-12-15T01:15:01.038529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-finetuning","metadata":{}},{"cell_type":"markdown","source":"### MRP Class","metadata":{}},{"cell_type":"code","source":"# This class creates a new BERT instance and initializes a MlP head on top.\n# The classifier head predicts the masked predictions for rationales\nclass Bert_MRP(nn.Module):\n\n    def __init__(self, n_classes=1):\n        super(Bert_MRP, self).__init__()\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states=True, output_attentions=True)\n        #self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n        self.out =  nn.Sequential(\n                              nn.Linear(self.bert.config.hidden_size, 512),\n                              nn.ReLU(),\n                              nn.Linear(512, 512),\n                              nn.ReLU(),\n                              nn.Dropout(p=0.3),\n                              nn.Linear(512, 1)\n                            )\n        self.criterion= nn.BCELoss()\n        self.tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n        \n    def forward(self, combined_encoding, attention_mask=None):\n        output = self.bert.encoder(combined_encoding, attention_mask = attention_mask)\n        return torch.sigmoid(self.out(output.last_hidden_state))\n    \n    def get_criterion(self):\n        return self.criterion\n    \n    \n    def assign_optimizer(self, **kwargs):\n        # TODO: your work below\n        return torch.optim.RAdam(self.parameters(),**kwargs)\n    \n    def tokenize(\n        self,\n        tok: \"list[str]\",\n        max_length: int = 128,\n        truncation: bool = True,\n        padding: bool = True,\n    ):\n        return self.tokenizer(tok, return_tensors='pt', is_split_into_words=True, padding='longest').to(device)\n    \n    def slice_cls_hidden_state(\n        self, x: transformers.modeling_outputs.BaseModelOutput\n    ) -> torch.Tensor:\n        return torch.stack([i[0] for i in x.last_hidden_state])","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:01.084718Z","iopub.execute_input":"2022-12-15T01:15:01.084987Z","iopub.status.idle":"2022-12-15T01:15:01.096450Z","shell.execute_reply.started":"2022-12-15T01:15:01.084962Z","shell.execute_reply":"2022-12-15T01:15:01.095455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### MRP Train Loop","metadata":{}},{"cell_type":"code","source":"# Training function for the intermediate MRP stage\ndef train_bertMRP(model, loader, device):\n    #criterion = model.get_criterion()\n    total_loss = 0.0\n    c=0\n    for tokens, rationale, target in tqdm(loader()):\n        c+=1\n        optimizer.zero_grad()\n\n        inputs = model.tokenize(tokens).to(device)\n        \n        rationale_in = rationale\n        \n        \n        rationale_in,sep_id=get_token_rationales(tokens,rationale_in)\n        rationale_token = model.tokenize(rationale_in).to(device)\n        \n                    \n        with torch.no_grad():\n            output_input = model.bert(**inputs)\n            output_rat = model.bert(**rationale_token)\n            attention = output_input[\"attentions\"][0]\n            combined_encoding,masked_indices=mask_emb(output_input[\"hidden_states\"][0],output_rat[\"hidden_states\"][0],sep_id, p =0.5)\n\n        \n        combined_encoding = combined_encoding.to(device)\n        pred = model(combined_encoding, attention_mask = attention)\n        \n        rationale_l = [model.tokenizer.convert_ids_to_tokens(ids) for ids in rationale_token['input_ids']]\n        \n        for r in rationale_l:\n            for i in range(len(r)):\n                if r[i]!='1' and r[i]!='0':\n                    r[i] = 0\n                else:\n                    r[i] = int(r[i])\n\n        rationale_l=torch.tensor(rationale_l, dtype=torch.float).to(device)\n        pred=torch.squeeze(pred).to(device)\n        \n        weights = rationale_l.clone()\n        for i in range(len(weights)):\n            for j in range(len(weights[i])):\n                if j not in masked_indices[i]:\n                    weights[i,j] = 0\n                else: \n                    weights[i,j] = 1\n        \n        criterion = nn.BCELoss(weight=weights)\n        loss = criterion(pred, rationale_l)\n\n        loss.backward()\n        optimizer.step() \n\n        total_loss += loss.item()\n        del pred\n        del rationale_in\n        del inputs\n        del combined_encoding\n        gc.collect()\n        torch.cuda.empty_cache()\n    return total_loss / c\n\n\n\n# Evaluation function for the intermediate MRP stage\n@torch.no_grad()\ndef eval_bertRP(model, loader, device):\n    model.eval()\n    criterion = model.get_criterion()\n    targets = []\n    preds = []\n    c=0\n    total_loss=0\n    for tokens, rationale, target in loader():\n        c+=1\n\n        inputs = model.tokenize(tokens).to(device)\n        \n        rationale_in = rationale\n        \n        \n        rationale_in,sep_id=get_token_rationales(tokens,rationale_in)\n        rationale_token = model.tokenize(rationale_in).to(device)\n        \n\n        output_input = model.bert(**inputs)\n        combined_encoding=output_input[\"hidden_states\"][0]\n        \n\n        combined_encoding = combined_encoding.to(device)\n        pred = model(combined_encoding, attention_mask = None)\n \n        preds+=pred\n        rationale_l = [model.tokenizer.convert_ids_to_tokens(ids) for ids in rationale_token['input_ids']]\n        \n        for r in rationale_l:\n            for i in range(len(r)):\n                if r[i]!='1' and r[i]!='0':\n                    r[i] = 0\n                else:\n                    r[i] = int(r[i])\n        targets+=rationale_in\n        loss = criterion(torch.squeeze(pred).to(device), torch.tensor(rationale_l, dtype=torch.float).to(device))\n\n        total_loss += loss.item()    \n    return preds, targets, total_loss/c    \n\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:17:54.272826Z","iopub.execute_input":"2022-12-15T01:17:54.273242Z","iopub.status.idle":"2022-12-15T01:17:54.293507Z","shell.execute_reply.started":"2022-12-15T01:17:54.273202Z","shell.execute_reply":"2022-12-15T01:17:54.292542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing loaders\nbatch_size=64\ntrain_loader=build_loader(HateXplainDataset(dataset[\"train\"]).data,batch_size=batch_size,shuffle=True)\nvalid_loader=build_loader(HateXplainDataset(dataset[\"validation\"]).data,batch_size=batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:01.157054Z","iopub.execute_input":"2022-12-15T01:15:01.157408Z","iopub.status.idle":"2022-12-15T01:15:07.882248Z","shell.execute_reply.started":"2022-12-15T01:15:01.157384Z","shell.execute_reply":"2022-12-15T01:15:07.881291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initializing training parameters\nn_epochs=20\nmodel_mrp=Bert_MRP()\nmodel_mrp=model_mrp.to(device)\nmodel_mrp.train()\noptimizer = model_mrp.assign_optimizer(lr=5e-5)\nv_loss_threshhold = 100\nv_loss = 99","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:07.883443Z","iopub.execute_input":"2022-12-15T01:15:07.885066Z","iopub.status.idle":"2022-12-15T01:15:14.555274Z","shell.execute_reply.started":"2022-12-15T01:15:07.885027Z","shell.execute_reply":"2022-12-15T01:15:14.554054Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training loop for MRP training stage\nfor epoch in range(n_epochs):\n        v_loss_threshhold = v_loss\n        print(\"Epoch:\", epoch)\n        loss = train_bertMRP(model_mrp, train_loader, device=device)\n        preds, targets, v_loss = eval_bertRP(model_mrp, valid_loader, device=device)\n     \n        print(\"Training loss:\", loss)\n        print(\"Validation loss:\", v_loss)\n        \n        preds = [x.tolist()[1:len(y)+1] for x,y in zip(preds, targets)]\n\n        pred_prob = [[i[0] for i in nested] for nested in preds]\n        preds = [[int(round(i[0])) for i in nested] for nested in preds]\n        \n        preds =[element for sublist in preds for element in sublist]\n        pred_prob = [element for sublist in pred_prob for element in sublist]\n        targets = [int(element) for sublist in targets for element in sublist]\n        print(\"Avg precision\", average_precision_score(targets, pred_prob))\n        precision, recall, thresholds = precision_recall_curve(preds, targets)\n        auc_precision_recall = auc(recall, precision)\n        epoch+=1\n        \n        print(\"AUCPR\",auc_precision_recall)\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:17:58.729574Z","iopub.execute_input":"2022-12-15T01:17:58.729950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Finetunning","metadata":{}},{"cell_type":"markdown","source":"### Final Classifier Class","metadata":{}},{"cell_type":"code","source":"# This class initializes using the BERT created from the previous intermediate stage\n# It creates a new head on top of BERT, and classifies input into the final three labels\n# Output classes: \"Normal\", \"Offensive\", \"Hate Speech\"\nclass BERT_HSD(nn.Module):\n\n    def __init__(self, bert):\n        super(BERT_HSD, self).__init__()\n        self.bert = bert \n        #self.out = nn.Linear(self.bert.config.hidden_size, 3)\n        self.out =  nn.Sequential(\n                              nn.Linear(self.bert.config.hidden_size, 512),\n                              nn.ReLU(),\n                              nn.Linear(512, 512),\n                              nn.ReLU(),\n                              nn.Dropout(p=0.2),\n                              nn.Linear(512, 3)\n                            )\n        self.softmax = nn.LogSoftmax(dim=1)\n        self.tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n        self.criterion= nn.CrossEntropyLoss()\n        \n    def slice_cls_hidden_state(\n        self, x: transformers.modeling_outputs.BaseModelOutput\n    ) -> torch.Tensor:\n        return torch.stack([i[0] for i in x.last_hidden_state])\n    \n    def get_criterion(self):\n        return self.criterion\n    \n    def assign_optimizer(self, **kwargs):\n        # TODO: your work below\n        return torch.optim.RAdam(self.parameters(),**kwargs)\n    \n    def tokenize(\n        self,\n        tok: \"list[str]\",\n        max_length: int = 128,\n        truncation: bool = True,\n        padding: bool = True,\n    ):\n        return tokenizer(tok, return_tensors='pt', is_split_into_words=True, padding='longest')\n\n    def forward(self, inputs):\n        #pass the inputs to the model  \n        x = self.bert(**inputs)\n        x = self.slice_cls_hidden_state(x)\n        x = self.out(x)\n        # apply softmax activation\n        x = self.softmax(x)\n        return x#torch.reshape(x,(-1,))","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:16.871571Z","iopub.status.idle":"2022-12-15T01:15:16.872345Z","shell.execute_reply.started":"2022-12-15T01:15:16.872081Z","shell.execute_reply":"2022-12-15T01:15:16.872105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize Loaders\nbatch_size=64\ntrain_loader=build_loader(HateXplainDataset(dataset[\"train\"]).data,batch_size=batch_size,shuffle=True)\nvalid_loader=build_loader(HateXplainDataset(dataset[\"validation\"]).data,batch_size=batch_size,shuffle=False)\ntest_loader=build_loader(HateXplainDataset(dataset[\"test\"]).data,batch_size=batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:16.873707Z","iopub.status.idle":"2022-12-15T01:15:16.874465Z","shell.execute_reply.started":"2022-12-15T01:15:16.874205Z","shell.execute_reply":"2022-12-15T01:15:16.874240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training dunction for the final finetuning stage\ndef train_berthsd(model, loader, device):\n    criterion = model.get_criterion()\n    total_loss = 0.0\n    c=0\n    for token, rat, target in tqdm(loader()):\n        c+=1\n        optimizer.zero_grad()\n\n        inputs = model.tokenize(token).to(device)\n        \n        target= torch.tensor(target,dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n\n        pred = model(inputs)\n                \n        loss = criterion(pred, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / c\n\n# Evaluation loop used for validation and test datasets\n@torch.no_grad()\ndef eval_berthsd(model, loader, device):\n    model.eval()\n\n    targets = []\n    preds = []\n    outs = []\n    for token, rat, target in loader():\n        inputs = model.tokenize(token).to(device)\n        out=model(inputs)\n        _,pred=torch.max(out, dim = 1)   \n        preds.append(pred)\n        target= torch.tensor(target,dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n        targets.append(target)\n        outs.append(out)\n\n    return torch.cat(preds), torch.cat(targets), torch.exp(torch.cat(outs))\n","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:16.875862Z","iopub.status.idle":"2022-12-15T01:15:16.876612Z","shell.execute_reply.started":"2022-12-15T01:15:16.876361Z","shell.execute_reply":"2022-12-15T01:15:16.876387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initial training parameters\nn_epochs=10\nmodel=model_mrp.bert\nmodel_hsd=BERT_HSD(model)\nmodel_hsd=model_hsd.to(device)\noptimizer = model_hsd.assign_optimizer(lr=2e-6)\n\nv_score_threshhold = 0\nscore = 0.00000001\n\n# Finetuning training loop\nfor epoch in range(n_epochs):\n        v_score_threshhold = score\n        loss = train_berthsd(model_hsd, train_loader, device=device)\n\n        preds, targets,_= eval_berthsd(model_hsd, valid_loader, device=device)\n        #preds = preds.round()\n        score = accuracy_score(targets.cpu(), preds.cpu())\n        print(\"Epoch:\", epoch)\n        print(\"Training loss:\", loss)\n        print(\"Validation F1 score:\", score)\n        print()\n        \n        \n        \n        preds, targets, outs= eval_berthsd(model_hsd, test_loader, device=device)\n        acc_score = accuracy_score(targets.cpu(), preds.cpu())\n        auroc = roc_auc_score(targets.cpu(),outs.cpu(), multi_class='ovr')\n        mf1=f1_score(targets.cpu(), preds.cpu(), average='macro')\n        pr_curve = torchmetrics.PrecisionRecallCurve(task=\"multiclass\", num_classes=3)\n        precision, recall, thresholds = pr_curve(outs, targets)\n        auprc=np.average([auc(recall[i].cpu(), precision[i].cpu()) for i in range(3)])\n\n        print(\"Test Accuracy Score:\", acc_score)\n        print(\"Test AUROC:\", auroc)\n        print(\"Test Macro-F1:\", mf1)\n        print(\"Test AUPRC:\", auprc)\n        epoch+=1\n        \n        \n        \n        preds, targets, v_loss = eval_bertRP(model_mrp, valid_loader, device=device)\n     \n\n        print(\"Validation loss:\", v_loss)\n        \n        preds = [x.tolist()[1:len(y)+1] for x,y in zip(preds, targets)]\n        pred_prob = [[i[0] for i in nested] for nested in preds]\n        preds = [[int(round(i[0])) for i in nested] for nested in preds]\n        \n        preds =[element for sublist in preds for element in sublist]\n        pred_prob = [element for sublist in pred_prob for element in sublist]\n        targets = [int(element) for sublist in targets for element in sublist]\n        print(\"Avg precision\", average_precision_score(targets, pred_prob))\n\n        precision, recall, thresholds = precision_recall_curve(preds, targets)\n\n        auc_precision_recall = auc(recall, precision)\n        print()","metadata":{"execution":{"iopub.status.busy":"2022-12-15T01:15:16.878018Z","iopub.status.idle":"2022-12-15T01:15:16.878767Z","shell.execute_reply.started":"2022-12-15T01:15:16.878512Z","shell.execute_reply":"2022-12-15T01:15:16.878537Z"},"trusted":true},"execution_count":null,"outputs":[]}]}